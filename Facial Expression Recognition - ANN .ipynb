{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import getData, softmax, cost2, y2indicator, error_rate, relu\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate = 10e-6, reg = 10e-1, epochs = 10000, show_fig = False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        \n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "        \n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        T = y2indicator(Y)\n",
    "        \n",
    "        self.W1 = np.random.randn(D, self.M) / np.sqrt(D + self.M)\n",
    "        self.b1 = np.zeros(self.M)\n",
    "        self.W2 = np.random.randn(self.M, K) / np.sqrt(self.M + K)\n",
    "        self.b2 = np.zeros(K)\n",
    "        \n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            pY, Z = self.forward(X)\n",
    "            \n",
    "            self.W2 -= learning_rate*(Z.T.dot(pY - T) + reg*self.W2)\n",
    "            self.b2 -= learning_rate*(np.sum(pY-T, axis=0) + reg*self.b2)\n",
    "            \n",
    "            # dZ = (pY-T).dot(self.W2.T) * (Z>0) #RelU\n",
    "            dZ = (pY-T).dot(self.W2.T) * (1- Z * Z) #Tanh\n",
    "            \n",
    "            self.W1 -= learning_rate * (X.T.dot(dZ) + reg*self.W1)\n",
    "            self.b1 -= learning_rate * (np.sum(dZ, axis=0) + reg*self.b1)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                pYvalid, _ = self.forward(Xvalid)\n",
    "                c = cost2(Yvalid, pYvalid)\n",
    "                costs.append(c)\n",
    "                predictions = np.argmax(pYvalid, axis = 1)\n",
    "                e = error_rate(Yvalid, predictions)\n",
    "                \n",
    "                print(\"i: \", i, \"cost: \", c, \"error: \", e)   \n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "        print(\"Best validation Error:\", best_validation_error)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    def forward(self, X):\n",
    "        # Z = relu(X.dot(self.W1) + self.b1)\n",
    "        Z = np.tanh(X.dot(self.W1) + self.b1)\n",
    "        return softmax(Z.dot(self.W2) + self.b2), Z\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pY, _ = self.forward(X)\n",
    "        return np.argmax(pY, axis=1)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0 cost:  5.623618164431784 error:  0.769\n",
      "i:  10 cost:  22.90162956208472 error:  0.917\n",
      "i:  20 cost:  24.016515780197363 error:  0.86\n",
      "i:  30 cost:  19.254036278717948 error:  0.769\n",
      "i:  40 cost:  32.0142036324238 error:  0.877\n",
      "i:  50 cost:  23.56062124800415 error:  0.885\n",
      "i:  60 cost:  30.763590605515486 error:  0.877\n",
      "i:  70 cost:  34.67636349074022 error:  0.769\n",
      "i:  80 cost:  32.7961346189428 error:  0.86\n",
      "i:  90 cost:  23.230256320259013 error:  0.877\n",
      "i:  100 cost:  33.20977914532187 error:  0.866\n",
      "i:  110 cost:  26.730315671423988 error:  0.769\n",
      "i:  120 cost:  44.183496123389176 error:  0.769\n",
      "i:  130 cost:  29.272836277743526 error:  0.866\n",
      "i:  140 cost:  23.826071790404843 error:  0.866\n",
      "i:  150 cost:  32.43555254207792 error:  0.885\n",
      "i:  160 cost:  28.410113338223898 error:  0.826\n",
      "i:  170 cost:  24.92190502171649 error:  0.866\n",
      "i:  180 cost:  39.753503293827265 error:  0.885\n",
      "i:  190 cost:  30.904893810851043 error:  0.877\n",
      "i:  200 cost:  40.30345484132685 error:  0.866\n",
      "i:  210 cost:  29.550406684618924 error:  0.769\n",
      "i:  220 cost:  37.92619635544693 error:  0.877\n",
      "i:  230 cost:  37.55759811692612 error:  0.826\n",
      "i:  240 cost:  30.65396131622119 error:  0.885\n",
      "i:  250 cost:  24.44845283164302 error:  0.86\n",
      "i:  260 cost:  27.423130865706437 error:  0.769\n",
      "i:  270 cost:  28.335211911993675 error:  0.866\n",
      "i:  280 cost:  36.28110326380854 error:  0.885\n",
      "i:  290 cost:  33.04416522796092 error:  0.866\n",
      "i:  300 cost:  24.52812055453592 error:  0.877\n",
      "i:  310 cost:  24.407491483676544 error:  0.885\n",
      "i:  320 cost:  29.673077695416453 error:  0.866\n",
      "i:  330 cost:  22.30195875415195 error:  0.826\n",
      "i:  340 cost:  22.330443964424223 error:  0.885\n",
      "i:  350 cost:  37.161310812523645 error:  0.885\n",
      "i:  360 cost:  39.71519224622284 error:  0.826\n",
      "i:  370 cost:  27.21332468346451 error:  0.877\n",
      "i:  380 cost:  18.741321203113564 error:  0.866\n",
      "i:  390 cost:  28.973695485062127 error:  0.769\n",
      "i:  400 cost:  35.69458832058473 error:  0.86\n",
      "i:  410 cost:  19.38188855458514 error:  0.877\n",
      "i:  420 cost:  28.625647118586443 error:  0.86\n",
      "i:  430 cost:  24.590702744834733 error:  0.769\n",
      "i:  440 cost:  27.168505940294434 error:  0.866\n",
      "i:  450 cost:  36.62797155303232 error:  0.917\n",
      "i:  460 cost:  24.817792949044186 error:  0.86\n",
      "i:  470 cost:  30.307979281620486 error:  0.917\n",
      "i:  480 cost:  33.14505690926003 error:  0.769\n",
      "i:  490 cost:  37.676180505788366 error:  0.877\n",
      "i:  500 cost:  34.937833299851626 error:  0.866\n",
      "i:  510 cost:  21.445815679887804 error:  0.826\n",
      "i:  520 cost:  25.774131816713787 error:  0.769\n",
      "i:  530 cost:  29.344044165326864 error:  0.885\n",
      "i:  540 cost:  25.24750132353674 error:  0.877\n",
      "i:  550 cost:  26.797818948963716 error:  0.769\n",
      "i:  560 cost:  29.60156122730483 error:  0.866\n",
      "i:  570 cost:  21.455448171915275 error:  0.917\n",
      "i:  580 cost:  28.42341347700687 error:  0.826\n",
      "i:  590 cost:  31.915221656100588 error:  0.917\n",
      "i:  600 cost:  26.533460800040178 error:  0.877\n",
      "i:  610 cost:  34.72857564495659 error:  0.877\n",
      "i:  620 cost:  27.963690110752186 error:  0.866\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X, Y, Xv, Yv = getData()\n",
    "    \n",
    "    model = ANN(200)\n",
    "    model.fit(X, Y , show_fig = True)\n",
    "    print (model.score())\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
